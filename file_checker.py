import array
import math
import os
import pickle

import joblib
import pefile

def get_entropy(data):
    if len(data) == 0:
        return 0.0  # Return 0.0 if the data is empty
    
    occurrences = array.array('L', [0] * 256)  # Initialize an array to count occurrences of each byte value (0-255)
    
    for x in data:
        occurrences[x if isinstance(x, int) else ord(x)] += 1  # Increment the count for the byte value
    
    entropy = 0  # Initialize entropy to 0
    
    for x in occurrences:
        if x:  # Only consider non-zero occurrences
            p_x = float(x) / len(data)  # Calculate the probability of the byte value
            entropy -= p_x * math.log(p_x, 2)  # Update entropy using Shannon entropy formula
    
    return entropy  # Return the calculated entropy

def get_resources(pe):
    resources = []  # Initialize a list to hold resources information
    if hasattr(pe, 'DIRECTORY_ENTRY_RESOURCE'):  # Check if the PE file has resource entries
        try:
            for resource_type in pe.DIRECTORY_ENTRY_RESOURCE.entries:
                if hasattr(resource_type, 'directory'):
                    for resource_id in resource_type.directory.entries:
                        if hasattr(resource_id, 'directory'):
                            for resource_lang in resource_id.directory.entries:
                                data = pe.get_data(resource_lang.data.struct.OffsetToData,
                                                   resource_lang.data.struct.Size)  # Get the resource data
                                size = resource_lang.data.struct.Size  # Get the size of the resource
                                entropy = get_entropy(data)  # Calculate the entropy of the resource data

                                resources.append([entropy, size])  # Append the entropy and size to the resources list
        except Exception as e:
            return resources  # Return the resources list if an exception occurs
    return resources  # Return the resources list

def get_version_info(pe):
    """Return version info's"""
    res = {}  # Initialize a dictionary to hold version information
    for fileinfo in pe.FileInfo:
        if fileinfo.Key == 'StringFileInfo':
            for st in fileinfo.StringTable:
                for entry in st.entries.items():
                    res[entry[0]] = entry[1]  # Add string file info entries to the dictionary
        if fileinfo.Key == 'VarFileInfo':
            for var in fileinfo.Var:
                res[var.entry.items()[0][0]] = var.entry.items()[0][1]  # Add variable file info entries to the dictionary
    if hasattr(pe, 'VS_FIXEDFILEINFO'):
        res['flags'] = pe.VS_FIXEDFILEINFO.FileFlags
        res['os'] = pe.VS_FIXEDFILEINFO.FileOS
        res['type'] = pe.VS_FIXEDFILEINFO.FileType
        res['file_version'] = pe.VS_FIXEDFILEINFO.FileVersionLS
        res['product_version'] = pe.VS_FIXEDFILEINFO.ProductVersionLS
        res['signature'] = pe.VS_FIXEDFILEINFO.Signature
        res['struct_version'] = pe.VS_FIXEDFILEINFO.StrucVersion  # Add fixed file info entries to the dictionary
    return res  # Return the version info dictionary

def extract_info(fpath):
    res = {}  # Initialize a dictionary to hold extracted information
    try:
        pe = pefile.PE(fpath)  # Load the PE file
    except pefile.PEFormatError:
        return {}  # Return an empty dictionary if there is a PE format error

    # Extract various attributes from the PE file headers
    res['Machine'] = pe.FILE_HEADER.Machine
    res['SizeOfOptionalHeader'] = pe.FILE_HEADER.SizeOfOptionalHeader
    res['Characteristics'] = pe.FILE_HEADER.Characteristics
    res['MajorLinkerVersion'] = pe.OPTIONAL_HEADER.MajorLinkerVersion
    res['MinorLinkerVersion'] = pe.OPTIONAL_HEADER.MinorLinkerVersion
    res['SizeOfCode'] = pe.OPTIONAL_HEADER.SizeOfCode
    res['SizeOfInitializedData'] = pe.OPTIONAL_HEADER.SizeOfInitializedData
    res['SizeOfUninitializedData'] = pe.OPTIONAL_HEADER.SizeOfUninitializedData
    res['AddressOfEntryPoint'] = pe.OPTIONAL_HEADER.AddressOfEntryPoint
    res['BaseOfCode'] = pe.OPTIONAL_HEADER.BaseOfCode
    
    try:
        res['BaseOfData'] = pe.OPTIONAL_HEADER.BaseOfData
    except AttributeError:
        res['BaseOfData'] = 0  # Set BaseOfData to 0 if not available

    res['ImageBase'] = pe.OPTIONAL_HEADER.ImageBase
    res['SectionAlignment'] = pe.OPTIONAL_HEADER.SectionAlignment
    res['FileAlignment'] = pe.OPTIONAL_HEADER.FileAlignment
    res['MajorOperatingSystemVersion'] = pe.OPTIONAL_HEADER.MajorOperatingSystemVersion
    res['MinorOperatingSystemVersion'] = pe.OPTIONAL_HEADER.MinorOperatingSystemVersion
    res['MajorImageVersion'] = pe.OPTIONAL_HEADER.MajorImageVersion
    res['MinorImageVersion'] = pe.OPTIONAL_HEADER.MinorImageVersion
    res['MajorSubsystemVersion'] = pe.OPTIONAL_HEADER.MajorSubsystemVersion
    res['MinorSubsystemVersion'] = pe.OPTIONAL_HEADER.MinorSubsystemVersion
    res['SizeOfImage'] = pe.OPTIONAL_HEADER.SizeOfImage
    res['SizeOfHeaders'] = pe.OPTIONAL_HEADER.SizeOfHeaders
    res['CheckSum'] = pe.OPTIONAL_HEADER.CheckSum
    res['Subsystem'] = pe.OPTIONAL_HEADER.Subsystem
    res['DllCharacteristics'] = pe.OPTIONAL_HEADER.DllCharacteristics
    res['SizeOfStackReserve'] = pe.OPTIONAL_HEADER.SizeOfStackReserve
    res['SizeOfStackCommit'] = pe.OPTIONAL_HEADER.SizeOfStackCommit
    res['SizeOfHeapReserve'] = pe.OPTIONAL_HEADER.SizeOfHeapReserve
    res['SizeOfHeapCommit'] = pe.OPTIONAL_HEADER.SizeOfHeapCommit
    res['LoaderFlags'] = pe.OPTIONAL_HEADER.LoaderFlags
    res['NumberOfRvaAndSizes'] = pe.OPTIONAL_HEADER.NumberOfRvaAndSizes

    # Extract sections information
    res['SectionsNb'] = len(pe.sections)  # Number of sections
    entropy = list(map(lambda x: x.get_entropy(), pe.sections))  # List of entropy values for each section
    res['SectionsMeanEntropy'] = sum(entropy) / float(len(entropy))  # Mean entropy of sections
    res['SectionsMinEntropy'] = min(entropy)  # Minimum entropy of sections
    res['SectionsMaxEntropy'] = max(entropy)  # Maximum entropy of sections
    raw_sizes = list(map(lambda x: x.SizeOfRawData, pe.sections))  # List of raw sizes for each section
    res['SectionsMeanRawsize'] = sum(raw_sizes) / float(len(raw_sizes))  # Mean raw size of sections
    res['SectionsMinRawsize'] = min(raw_sizes)  # Minimum raw size of sections
    res['SectionsMaxRawsize'] = max(raw_sizes)  # Maximum raw size of sections
    virtual_sizes = list(map(lambda x: x.Misc_VirtualSize, pe.sections))  # List of virtual sizes for each section
    res['SectionsMeanVirtualsize'] = sum(virtual_sizes) / float(len(virtual_sizes))  # Mean virtual size of sections
    res['SectionsMinVirtualsize'] = min(virtual_sizes)  # Minimum virtual size of sections
    res['SectionMaxVirtualsize'] = max(virtual_sizes)  # Maximum virtual size of sections

    try:
        res['ImportsNbDLL'] = len(pe.DIRECTORY_ENTRY_IMPORT)  # Number of imported DLLs
        imports = sum([x.imports for x in pe.DIRECTORY_ENTRY_IMPORT], [])  # Flatten list of imports
        res['ImportsNb'] = len(imports)  # Number of imports
        res['ImportsNbOrdinal'] = len(list(filter(lambda x: x.name is None, imports)))  # Number of ordinal imports
    except AttributeError:
        res['ImportsNbDLL'] = 0  # Set to 0 if no imports
        res['ImportsNb'] = 0  # Set to 0 if no imports
        res['ImportsNbOrdinal'] = 0  # Set to 0 if no ordinal imports

    try:
        res['ExportNb'] = len(pe.DIRECTORY_ENTRY_EXPORT.symbols)  # Number of exported symbols
    except AttributeError:
        res['ExportNb'] = 0  # Set to 0 if no exports

    # Resources information
    resources = get_resources(pe)
    res['ResourcesNb'] = len(resources)  # Number of resources
    if len(resources) > 0:
        entropy = list(map(lambda x: x[0], resources))  # List of entropies for resources
        res['ResourcesMeanEntropy'] = sum(entropy) / float(len(entropy))  # Mean entropy of resources
        res['ResourcesMinEntropy'] = min(entropy)  # Minimum entropy of resources
        res['ResourcesMaxEntropy'] = max(entropy)  # Maximum entropy of resources
        sizes = list(map(lambda x: x[1], resources))  # List of sizes for resources
        res['ResourcesMeanSize'] = sum(sizes) / float(len(sizes))  # Mean size of resources
        res['ResourcesMinSize'] = min(sizes)  # Minimum size of resources
        res['ResourcesMaxSize'] = max(sizes)  # Maximum size of resources
    else:
        # Set default values if there are no resources
        res['ResourcesNb'] = 0
        res['ResourcesMeanEntropy'] = 0
        res['ResourcesMinEntropy'] = 0
        res['ResourcesMaxEntropy'] = 0
        res['ResourcesMeanSize'] = 0
        res['ResourcesMinSize'] = 0
        res['ResourcesMaxSize'] = 0

    try:
        res['LoadConfigurationSize'] = pe.DIRECTORY_ENTRY_LOAD_CONFIG.struct.Size  # Size of the load configuration
    except AttributeError:
        res['LoadConfigurationSize'] = 0  # Set to 0 if no load configuration

    try:
        version_info = get_version_info(pe)  # Get version information
        res['VersionInformationSize'] = len(version_info.keys())  # Number of version information entries
    except AttributeError:
        res['VersionInformationSize'] = 0  # Set to 0 if no version information

    return res  # Return the dictionary of extracted information

def checkFile(file):
    model = joblib.load("model/model.pkl")  # Load the pre-trained model
    features = pickle.loads(open(os.path.join('model/features.pkl'), 'rb').read())  # Load the list of features
    data = extract_info(file)  # Extract information from the file

    if data != {}:
        pe_features = list(map(lambda x: data[x], features))  # Create a list of feature values from the extracted data
        res = model.predict([pe_features])[0]  # Predict using the model and get the result
    else:
        res = 1  # Return 1 (indicating an error or unknown) if data extraction failed

    return res  # Return the prediction result
